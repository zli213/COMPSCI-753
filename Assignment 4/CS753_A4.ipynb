{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'user_id': '26d5737b1eaff71248069cde4f590338', 'book_id': '30109111', 'review_id': 'd567c1be612401ee5cbe3da05683561f', 'rating': 5, 'date_added': 'Sun Jun 12 09:38:16 -0700 2016'}, {'user_id': 'e0a970290631fd711484f0d8155f2a06', 'book_id': '7198269', 'review_id': '1fca74b92a06f2cccdc81c1288687495', 'rating': 5, 'date_added': 'Thu May 10 18:37:25 -0700 2012'}, {'user_id': 'cca945e8a7369eeb035afd21527c339b', 'book_id': '32148570', 'review_id': 'e519d8377a10308742dd49f66f4a728a', 'rating': 3, 'date_added': 'Mon May 29 10:32:28 -0700 2017'}, {'user_id': 'd1789d248a75d3cb7c5f16eeee9fe419', 'book_id': '40024', 'review_id': 'a75d9f435773a377fbe81361a1ea19c6', 'rating': 2, 'date_added': 'Mon Jan 26 21:04:34 -0800 2009'}, {'user_id': '819f2797459b579a7782d4bd595e1c36', 'book_id': '3272163', 'review_id': 'cfbc4e10f33bad3bd235f775e1833b2d', 'rating': 3, 'date_added': 'Wed Jun 25 12:12:10 -0700 2014'}, {'user_id': '7d0b0d563843507c71f867720801d84e', 'book_id': '361056', 'review_id': '41556ee650e292fe914816abf9455062', 'rating': 1, 'date_added': 'Fri Oct 19 06:54:29 -0700 2007'}, {'user_id': 'b85e8348c1e1f6ed6ae8e76bb6de7f15', 'book_id': '10855973', 'review_id': 'be2a3aa400602d43bdbc5c5196235088', 'rating': 5, 'date_added': 'Thu May 17 13:29:51 -0700 2012'}, {'user_id': '17d3a7342670ec8c47777296bfe71709', 'book_id': '15802432', 'review_id': '398c3f730486a30af1cf9be55700d5d1', 'rating': 4, 'date_added': 'Sun Apr 14 07:38:58 -0700 2013'}, {'user_id': '750e558376102be2392d6f80a77c186f', 'book_id': '53645', 'review_id': '808757067ba063061a494142cbaeab1f', 'rating': 4, 'date_added': 'Mon Jun 09 10:53:05 -0700 2008'}, {'user_id': 'ac87d8e90ac47cd3cc69baf88857650e', 'book_id': '2318271', 'review_id': '8e0e4899a50359a17221a7d50e8c6890', 'rating': 5, 'date_added': 'Sun May 12 17:32:10 -0700 2013'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "train_data = read_data(\"goodreads_reviews_historybio_train.json\")\n",
    "# print the first 10 line of the data\n",
    "print(train_data[:10])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 [10 points]: Explore biases\n",
    "(A) [4 points] The global bg bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global bias bg: 3.7669762808387413\n"
     ]
    }
   ],
   "source": [
    "def calculate_global_bias(data):\n",
    "    total_rating = sum([review['rating'] for review in data])\n",
    "    return total_rating / len(data)\n",
    "\n",
    "bg = calculate_global_bias(train_data)\n",
    "print(f\"Global bias bg: {bg}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) [3 points] The user specific bias of user id= “3913f3be1e8fadc1de34dc49dab06381”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User bias for user 3913f3be1e8fadc1de34dc49dab06381: -0.1139150563489455\n"
     ]
    }
   ],
   "source": [
    "def calculate_user_bias(data, uid, global_bias):\n",
    "    user_reviews = [review for review in data if review['user_id'] == uid]\n",
    "    #calculate the average rating of the user\n",
    "    average_user_rating = sum([review['rating'] for review in user_reviews])/len(user_reviews)\n",
    "    return average_user_rating - global_bias\n",
    "user_id = \"3913f3be1e8fadc1de34dc49dab06381\"\n",
    "b_user = calculate_user_bias(train_data, user_id, bg)\n",
    "print(f\"User bias for user {user_id}: {b_user}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) [3 points] The item specific bias of book id = “16130”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item specific bias for book id 16130: 0.4562653093753264\n"
     ]
    }
   ],
   "source": [
    "def calculate_item_bias(data, book_id, global_bias):\n",
    "    item_reviews = [review for review in data if review['book_id'] == book_id]\n",
    "    total_item_bias = sum([review['rating'] for review in item_reviews])/ len(item_reviews)\n",
    "    return total_item_bias -global_bias if item_reviews else 0\n",
    "\n",
    "book_id = \"16130\"\n",
    "b_item = calculate_item_bias(train_data, book_id, bg)\n",
    "print(f\"Item specific bias for book id {book_id}: {b_item}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 [45 points]: Implement the regularized latent factor model without bias using SGD\n",
    "(A) [30 points] Implement the regularized latent factor model without considering the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: RMSE = 3.971009273832496\n",
      "Epoch 2: RMSE = 3.7346380322699684\n",
      "Epoch 3: RMSE = 2.9982514795639115\n",
      "Epoch 4: RMSE = 2.547222226771712\n",
      "Epoch 5: RMSE = 2.235647198449414\n",
      "Epoch 6: RMSE = 2.0094515101989767\n",
      "Epoch 7: RMSE = 1.8344387330227594\n",
      "Epoch 8: RMSE = 1.696898743950972\n",
      "Epoch 9: RMSE = 1.5849721950948181\n",
      "Epoch 10: RMSE = 1.4910604859689456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Initialization\n",
    "def initialization(k,train_data):\n",
    "    num_users = len(set([d['user_id'] for d in train_data]))\n",
    "    num_items = len(set([d['book_id'] for d in train_data]))\n",
    "    P = np.random.normal(scale=0.01,size=(num_users, k))\n",
    "    Q = np.random.normal(scale=0.01,size=(num_items, k))\n",
    "    # Mapping user_ids and book_ids to integer indices for easier array operations\n",
    "    user_map = {user_id: idx for idx, user_id in enumerate(set([d['user_id'] for d in train_data]))}\n",
    "    book_map = {book_id: idx for idx, book_id in enumerate(set([d['book_id'] for d in train_data]))}\n",
    "    return P,Q,user_map,book_map\n",
    "\n",
    "k = 8\n",
    "eta = 0.01\n",
    "lambda1 = lambda2 = 0.3\n",
    "epochs = 10\n",
    "P, Q, user_map, book_map = initialization(k,train_data)\n",
    "\n",
    "# 2. SGD\n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(train_data)\n",
    "    for review in train_data:\n",
    "        i = user_map[review['user_id']]\n",
    "        j = book_map[review['book_id']]\n",
    "        r_ij = review['rating']\n",
    "        e_ij = r_ij - np.dot(Q[j], P[i])\n",
    "\n",
    "        # Update using gradients\n",
    "        temp_q = Q[j, :]\n",
    "        Q[j, :] += 2*eta * (e_ij * P[i, :] - lambda1 * Q[j, :])\n",
    "        P[i, :] += 2*eta * (e_ij * temp_q - lambda2 * P[i, :])\n",
    "\n",
    "    # 3. RMSE Calculation\n",
    "    squared_errors = []\n",
    "    for review in train_data:\n",
    "        i = user_map[review['user_id']]\n",
    "        j = book_map[review['book_id']]\n",
    "        r_ij = review['rating']\n",
    "        squared_errors.append((r_ij - np.dot(Q[j], P[i])) ** 2)\n",
    "    rmse = np.sqrt(sum(squared_errors) / len(squared_errors))\n",
    "    print(f\"Epoch {epoch+1}: RMSE = {rmse}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) [15 points] Use SGD to train the latent factor model on the training data for different values of k in {4,8,16}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 4, Validation RMSE: 1.5069\n",
      "k = 8, Validation RMSE: 1.4869\n",
      "k = 16, Validation RMSE: 1.4745\n",
      "Best k = 16, Test RMSE: 1.5992\n"
     ]
    }
   ],
   "source": [
    "def train_latent_factors(P, Q, user_map, book_map, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        np.random.shuffle(train_data)\n",
    "        for review in train_data:\n",
    "            i = user_map[review['user_id']]\n",
    "            j = book_map[review['book_id']]\n",
    "            r_ij = review['rating']\n",
    "            e_ij = r_ij - np.dot(Q[j], P[i])\n",
    "\n",
    "            # Update using gradients\n",
    "            temp_q = Q[j, :]\n",
    "            Q[j, :] += 2*eta * (e_ij * P[i, :] - lambda1 * Q[j, :])\n",
    "            P[i, :] += 2*eta * (e_ij * temp_q - lambda2 * P[i, :])\n",
    "\n",
    "    return P, Q\n",
    "\n",
    "def compute_rmse(data, P, Q, user_map, book_map):\n",
    "    squared_errors = []\n",
    "    for review in data:\n",
    "        if review['user_id'] not in user_map or review['book_id'] not in book_map:\n",
    "            continue  # Skip this review if user or book not found in the maps\n",
    "        i = user_map[review['user_id']]\n",
    "        j = book_map[review['book_id']]\n",
    "        r_ij = review['rating']\n",
    "        squared_errors.append((r_ij - np.dot(Q[j], P[i])) ** 2)\n",
    "    rmse = np.sqrt(sum(squared_errors) / len(squared_errors))\n",
    "    return rmse\n",
    "\n",
    "# Using the functions\n",
    "best_rmse = float('inf') # set it to positive infinity\n",
    "best_k = None\n",
    "best_P = None\n",
    "best_Q = None\n",
    "\n",
    "for k in [4, 8, 16]:\n",
    "    P, Q, user_map, book_map = initialization(k, train_data)\n",
    "    P, Q = train_latent_factors(P, Q, user_map, book_map)\n",
    "    validation_rmse = compute_rmse(train_data, P, Q, user_map, book_map)\n",
    "    print(f\"k = {k}, Validation RMSE: {validation_rmse:.4f}\")\n",
    "\n",
    "    if validation_rmse < best_rmse:\n",
    "        best_rmse = validation_rmse\n",
    "        best_k = k\n",
    "        best_P = P\n",
    "        best_Q = Q\n",
    "test_data = read_data(\"goodreads_reviews_historybio_test.json\")\n",
    "test_rmse = compute_rmse(test_data, best_P, best_Q, user_map, book_map)\n",
    "print(f\"Best k = {best_k}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 [45 points]: Implement the regularized latent factor model with bias using SGD\n",
    "(A) [30 points] Incorporate the bias terms bg, b(user) and b(item) to the latent factor model. ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: RMSE = 1.1096063055209227\n",
      "Epoch 2: RMSE = 1.0664791285164892\n",
      "Epoch 3: RMSE = 1.0671081198948262\n",
      "Epoch 4: RMSE = 1.0499299164387785\n",
      "Epoch 5: RMSE = 1.0449913828986686\n",
      "Epoch 6: RMSE = 1.0431716040150023\n",
      "Epoch 7: RMSE = 1.0415781334173932\n",
      "Epoch 8: RMSE = 1.0331426353178252\n",
      "Epoch 9: RMSE = 1.0450186464762201\n",
      "Epoch 10: RMSE = 1.032782589963173\n",
      "User bias for user 3913f3be1e8fadc1de34dc49dab06381: -0.26721905308528415\n",
      "Item specific bias for book id 16130: -0.26721905308528415\n"
     ]
    }
   ],
   "source": [
    "lambda1 = lambda2 = lambda3 =lambda4 =0.3\n",
    "user_id = \"3913f3be1e8fadc1de34dc49dab06381\"\n",
    "book_id = \"16130\"\n",
    "bg = calculate_global_bias(train_data)\n",
    "b_user = calculate_user_bias(train_data, user_id, bg)\n",
    "b_item = calculate_item_bias(train_data, book_id, bg)\n",
    "\n",
    "def train_latent_factors_with_bias(lambda4, lambda3, lambda2, lambda1, bg, b_user, b_item, data):\n",
    "    np.random.shuffle(data)\n",
    "    for review in data:\n",
    "        i = user_map[review['user_id']]\n",
    "        j = book_map[review['book_id']]\n",
    "        r_ij_actual = review['rating']\n",
    "        r_ij_predicted = bg + b_user + b_item + np.dot(Q[j], P[i])\n",
    "        e_ij = r_ij_actual - r_ij_predicted\n",
    "\n",
    "        # Update using gradients\n",
    "        temp_q = Q[j, :]\n",
    "        Q[j, :] += 2*eta * (e_ij * P[i, :] - lambda1 * Q[j, :])\n",
    "        P[i, :] += 2*eta * (e_ij * temp_q - lambda2 * P[i, :])\n",
    "\n",
    "        # Update biases\n",
    "        b_user += 2*eta * (e_ij - lambda3 * b_user)\n",
    "        b_item += 2*eta * (e_ij - lambda4 * b_item)\n",
    "    return P, Q, b_user, b_item\n",
    "def compute_rmse(P,Q, bg, b_user, b_item, data):\n",
    "    squared_errors = []\n",
    "    for review in train_data:\n",
    "        i = user_map[review['user_id']]\n",
    "        j = book_map[review['book_id']]\n",
    "        r_ij = review['rating']\n",
    "        squared_errors.append((r_ij - (bg + b_user + b_item + np.dot(Q[j], P[i]))) ** 2)\n",
    "    rmse = np.sqrt(sum(squared_errors) / len(squared_errors))\n",
    "    return rmse\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    P, Q, b_user, b_item = train_latent_factors_with_bias(lambda4, lambda3, lambda2, lambda1, bg, b_user, b_item,train_data)\n",
    "\n",
    "    # 3. RMSE Calculation\n",
    "    # squared_errors = []\n",
    "    # for review in train_data:\n",
    "    #     i = user_map[review['user_id']]\n",
    "    #     j = book_map[review['book_id']]\n",
    "    #     r_ij = review['rating']\n",
    "    #     squared_errors.append((r_ij - (bg + b_user + b_item + np.dot(Q[j], P[i]))) ** 2)\n",
    "    # rmse = np.sqrt(sum(squared_errors) / len(squared_errors))\n",
    "    rmse = compute_rmse(P,Q, bg, b_user, b_item, train_data)\n",
    "    print(f\"Epoch {epoch+1}: RMSE = {rmse}\")\n",
    "# After finishing all epoches, report the learned user-specific bias of the user with user id= “3913f3be1e8fadc1de34dc49dab06381” , and the learned item- specific bias of the book with book id = “16130”.\n",
    "\n",
    "print(f\"User bias for user {user_id}: {b_user}\")\n",
    "print(f\"Item specific bias for book id {book_id}: {b_item}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) [15 points] Similar to Task 2 (B), find the best k in {4, 8, 16} for the model you developed in Task 3 (A) on the validation set, by using RMSE to compare across these models, and apply the best of these models to the test data. Compare the resulting test RMSE with Task 2 (B). Analyse and explain your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k for Task 3 is 4, with a test RMSE of 1.2667078269360939\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "    return data\n",
    "\n",
    "best_k = None\n",
    "best_rmse = float('inf')\n",
    "best_P = None\n",
    "best_Q = None\n",
    "validation_data = read_data(\"goodreads_reviews_historybio_val.json\")\n",
    "test_data = read_data(\"goodreads_reviews_historybio_test.json\")\n",
    "\n",
    "for k in [4, 8, 16]:\n",
    "    P, Q, user_map, book_map = initialization(k, train_data)\n",
    "    \n",
    "    # Use the above train function to train the model\n",
    "    P, Q, b_user, b_item = train_latent_factors_with_bias(lambda4, lambda3, lambda2, lambda1, bg, b_user, b_item,train_data)\n",
    "    \n",
    "    # Calculate RMSE on the validation data\n",
    "    validation_rmse = compute_rmse(P,Q, bg, b_user, b_item,validation_data)\n",
    "    \n",
    "    # Update best RMSE and k\n",
    "    if validation_rmse < best_rmse:\n",
    "        best_rmse = validation_rmse\n",
    "        best_k = k\n",
    "        best_P = P\n",
    "        best_Q = Q\n",
    "\n",
    "# Calculate RMSE on the test data with the best k\n",
    "test_rmse = compute_rmse(best_P, best_Q, bg, b_user, b_item, test_data)\n",
    "\n",
    "print(f\"Best k for Task 3 is {best_k}, with a test RMSE of {test_rmse}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "1. **Lower RMSE in Task 3(B) compared to Task 2(B):**\n",
    "    - The inclusion of bias terms in the latent factor model (Task 3(B)) led to a better fit to the data, resulting in a lower RMSE. This suggests that incorporating user and item-specific biases can capture inherent characteristics in the data, enhancing prediction accuracy.\n",
    "\n",
    "2. **Higher optimal \\( k \\) value in Task 2(B) than in Task 3(B):**\n",
    "    - The model in Task 3(B) required a smaller number of latent factors \\( k \\) to achieve optimal performance, likely because the added bias terms already captured some intrinsic properties of users and items. This indicates that the bias-enhanced model can achieve comparable or better performance using fewer latent factors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
